{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Advanced Dictionary Serialization & Performance\n",
    "\n",
    "In Notebook 1, we covered the basics of serializing dictionaries using JSON, Pickle, YAML, and MessagePack. Now, let's tackle more complex scenarios and explore how to optimize serialization performance.\n",
    "\n",
    "**Learning Objectives:**\n",
    "*   Handle custom objects within dictionaries during JSON serialization.\n",
    "*   Understand challenges with circular references.\n",
    "*   Compare performance (speed, size) of different serialization formats.\n",
    "*   Learn techniques like incremental serialization and compression.\n",
    "*   See how serialization is used for caching.\n",
    "*   Compare Pickle and HDF5 for specific high-performance use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Advanced Dictionary Serialization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Complex Dictionary Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Custom Objects in Dictionaries with JSON**\n",
    "\n",
    "Standard JSON serializers (`json.dumps`) don't know how to handle custom Python objects or types like `datetime`. We need to provide custom encoders and decoders.\n",
    "\n",
    "**Encoding:** Subclass `json.JSONEncoder` and override the `default` method.\n",
    "**Decoding:** Use the `object_hook` parameter in `json.loads`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "# Example dictionary with a datetime object (not directly JSON serializable)\n",
    "event_dict = {\n",
    "    'event_id': 'conf_2025',\n",
    "    'name': 'Annual Tech Conference',\n",
    "    'start_date': datetime(2025, 10, 20, 9, 0, 0),\n",
    "    'attendees': ['Alice', 'Bob', 'Charlie']\n",
    "}\n",
    "\n",
    "print(\"Original Dictionary with datetime:\")\n",
    "pprint(event_dict)\n",
    "\n",
    "# --- Attempt standard JSON serialization (will fail) ---\n",
    "try:\n",
    "    json.dumps(event_dict)\n",
    "except TypeError as e:\n",
    "    print(f\"\\nStandard JSON failed as expected: {e}\")\n",
    "\n",
    "# --- Custom Encoder --- \n",
    "class DateTimeEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, datetime):\n",
    "            # Represent datetime as an ISO 8601 string\n",
    "            return {'__datetime__': obj.isoformat()} # Add hint for decoder\n",
    "        # Let the base class default method raise the TypeError for other types\n",
    "        return super().default(obj)\n",
    "\n",
    "print(\"\\nSerializing with custom encoder...\")\n",
    "json_string_custom = json.dumps(event_dict, cls=DateTimeEncoder, indent=2)\n",
    "print(json_string_custom)\n",
    "\n",
    "# --- Custom Decoder (using object_hook) --- \n",
    "def datetime_decoder(dct):\n",
    "    if '__datetime__' in dct:\n",
    "        # Convert back to datetime object\n",
    "        return datetime.fromisoformat(dct['__datetime__'])\n",
    "    return dct # Return other dictionaries unchanged\n",
    "\n",
    "print(\"\\nDeserializing with custom decoder...\")\n",
    "reconstructed_custom = json.loads(json_string_custom, object_hook=datetime_decoder)\n",
    "pprint(reconstructed_custom)\n",
    "\n",
    "# Verify the datetime object was correctly reconstructed\n",
    "assert isinstance(reconstructed_custom['start_date'], datetime)\n",
    "assert event_dict == reconstructed_custom\n",
    "\n",
    "print(\"\\nSuccessfully serialized and deserialized dictionary with datetime object!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Nested Dictionaries and Circular References**\n",
    "\n",
    "Dictionaries containing references to themselves or other objects that refer back create circular references. Standard recursive serializers (like `json.dumps`) will enter an infinite loop and raise a `RecursionError`.\n",
    "\n",
    "*   **Pickle:** Handles circular references automatically.\n",
    "*   **JSON/YAML/Others:** Require custom logic to detect and handle cycles, often by replacing repeated object references with a special marker or ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "# Create a dictionary with a circular reference\n",
    "circular_dict = {'name': 'self-referential', 'value': 42}\n",
    "circular_dict['myself'] = circular_dict # The circular reference!\n",
    "\n",
    "# --- Attempt standard JSON serialization (will fail) ---\n",
    "print(\"Attempting JSON serialization on circular dictionary:\")\n",
    "try:\n",
    "    json.dumps(circular_dict)\n",
    "except RecursionError as e:\n",
    "    print(f\"  >>> Failed with RecursionError (or ValueError in some versions) as expected: {e}\")\n",
    "except ValueError as e: # Newer Python versions might raise ValueError\n",
    "    print(f\"  >>> Failed with ValueError as expected: {e}\")\n",
    "    \n",
    "# --- Pickle serialization (works!) ---\n",
    "print(\"\\nAttempting Pickle serialization on circular dictionary:\")\n",
    "try:\n",
    "    pickled_circular = pickle.dumps(circular_dict)\n",
    "    print(f\"  >>> Pickle succeeded! Size: {len(pickled_circular)} bytes\")\n",
    "    \n",
    "    # Deserialization also works\n",
    "    unpickled_circular = pickle.loads(pickled_circular)\n",
    "    assert unpickled_circular['name'] == 'self-referential'\n",
    "    assert unpickled_circular['myself'] is unpickled_circular # The reference is restored\n",
    "    print(\"  >>> Pickle deserialization restored the circular reference.\")\n",
    "except Exception as e:\n",
    "    print(f\"  >>> Pickle failed unexpectedly: {e}\")\n",
    "    \n",
    "# Note: Handling circular refs in JSON requires complex custom logic (beyond scope here)\n",
    "# involving tracking object ids ('seen' set) and using placeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Optimization for Dictionary Serialization\n",
    "\n",
    "When dealing with large dictionaries or frequent serialization, performance matters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Binary Serialization for Speed and Size**\n",
    "\n",
    "Binary formats like MessagePack (or Protocol Buffers, not shown here) are generally faster and produce smaller output than text-based formats like JSON, especially for numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Requires msgpack: pip install msgpack-python\n",
    "import json\n",
    "import msgpack\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Create a reasonably large dictionary\n",
    "num_items = 100000\n",
    "large_dict = {f'key_{i}': f'value_long_string_{i}' for i in range(num_items)}\n",
    "\n",
    "print(f\"Comparing performance for a dictionary with {num_items} items:\\n\")\n",
    "\n",
    "# --- Measure JSON --- \n",
    "start_time = time.perf_counter()\n",
    "json_data = json.dumps(large_dict).encode('utf-8') # Encode to bytes for fair size comparison\n",
    "json_time = time.perf_counter() - start_time\n",
    "json_size = sys.getsizeof(json_data) # More accurate memory size\n",
    "\n",
    "start_time_des = time.perf_counter()\n",
    "json_des = json.loads(json_data.decode('utf-8'))\n",
    "json_time_des = time.perf_counter() - start_time_des\n",
    "\n",
    "print(f\"JSON Serialization:   {json_time:.4f}s, Size: {json_size / 1024:.2f} KB\")\n",
    "print(f\"JSON Deserialization: {json_time_des:.4f}s\")\n",
    "\n",
    "# --- Measure MessagePack --- \n",
    "try:\n",
    "    start_time = time.perf_counter()\n",
    "    msgpack_data = msgpack.packb(large_dict)\n",
    "    msgpack_time = time.perf_counter() - start_time\n",
    "    msgpack_size = sys.getsizeof(msgpack_data)\n",
    "    \n",
    "    start_time_des = time.perf_counter()\n",
    "    msgpack_des = msgpack.unpackb(msgpack_data, raw=False)\n",
    "    msgpack_time_des = time.perf_counter() - start_time_des\n",
    "\n",
    "    print(f\"\\nMessagePack Serialization:   {msgpack_time:.4f}s, Size: {msgpack_size / 1024:.2f} KB\")\n",
    "    print(f\"MessagePack Deserialization: {msgpack_time_des:.4f}s\")\n",
    "\n",
    "    # --- Measure Pickle --- \n",
    "    # Use highest protocol for best performance\n",
    "    start_time = time.perf_counter()\n",
    "    pickle_data = pickle.dumps(large_dict, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    pickle_time = time.perf_counter() - start_time\n",
    "    pickle_size = sys.getsizeof(pickle_data)\n",
    "    \n",
    "    start_time_des = time.perf_counter()\n",
    "    pickle_des = pickle.loads(pickle_data)\n",
    "    pickle_time_des = time.perf_counter() - start_time_des\n",
    "\n",
    "    print(f\"\\nPickle Serialization:   {pickle_time:.4f}s, Size: {pickle_size / 1024:.2f} KB\")\n",
    "    print(f\"Pickle Deserialization: {pickle_time_des:.4f}s\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\nMessagePack test skipped: msgpack not installed.\")\n",
    "except Exception as e:\n",
    "     print(f\"\\nAn error occurred during performance test: {e}\")\n",
    "\n",
    "# Note: Actual performance varies greatly based on data complexity, Python version, and hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Incremental Serialization for Large Dictionaries**\n",
    "\n",
    "If a dictionary is too large to fit into memory all at once during serialization (e.g., loading from a huge database or generator), you might need to serialize it incrementally. This is easier with formats that support streaming.\n",
    "\n",
    "For JSON, this often means manually constructing the JSON structure piece by piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Simulate generating key-value pairs for a huge dictionary\n",
    "def generate_large_dict_items(n):\n",
    "    print(f\"Simulating generation of {n} items...\")\n",
    "    for i in range(n):\n",
    "        # Simulate some work to get the item\n",
    "        yield f\"item_key_{i}\", {'value': i*i, 'status': 'generated'}\n",
    "        if (i+1) % 10000 == 0:\n",
    "            print(f\"  ...generated {i+1} items\")\n",
    "            \n",
    "# File to write to\n",
    "output_filename = 'large_incremental_data.json'\n",
    "num_items_to_generate = 50000 # Smaller number for demo\n",
    "\n",
    "print(f\"Writing {num_items_to_generate} items incrementally to {output_filename}\\n\")\n",
    "\n",
    "try:\n",
    "    with open(output_filename, 'w') as f:\n",
    "        f.write('{') # Start JSON object\n",
    "        first_item = True\n",
    "        for key, value in generate_large_dict_items(num_items_to_generate):\n",
    "            if not first_item:\n",
    "                f.write(',\\n ') # Add comma and newline for readability\n",
    "            else:\n",
    "                f.write('\\n ') # Start first item on new line\n",
    "                first_item = False\n",
    "            \n",
    "            # Manually serialize key and value\n",
    "            # Ensure key is properly escaped JSON string\n",
    "            f.write(f'{json.dumps(key)}: {json.dumps(value)}') \n",
    "            \n",
    "        f.write('\\n}') # End JSON object\n",
    "    print(f\"\\nSuccessfully wrote {output_filename}\")\n",
    "\n",
    "    # Clean up the file afterwards for demo purposes\n",
    "    # os.remove(output_filename)\n",
    "    # print(f\"Cleaned up {output_filename}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during incremental write: {e}\")\n",
    "\n",
    "# Note: Deserializing such a large file might also require streaming parsers (e.g., ijson library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Compression for Serialized Data**\n",
    "\n",
    "Combine serialization with compression libraries like `gzip` or `bz2` to save disk space or reduce network bandwidth, especially for text-based formats like JSON or verbose binary formats.\n",
    "\n",
    "*Trade-off:* Compression adds CPU overhead during both serialization and deserialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Use the large dictionary from the performance test\n",
    "num_items = 100000\n",
    "large_dict = {f'key_{i}': f'value_long_string_{i}' for i in range(num_items)}\n",
    "\n",
    "output_file_pkl = 'large_dict.pkl'\n",
    "output_file_pkl_gz = 'large_dict.pkl.gz'\n",
    "\n",
    "# --- Standard Pickle --- \n",
    "print(\"Serializing with standard Pickle...\")\n",
    "start = time.perf_counter()\n",
    "with open(output_file_pkl, 'wb') as f:\n",
    "    pickle.dump(large_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "time_pkl = time.perf_counter() - start\n",
    "size_pkl = os.path.getsize(output_file_pkl)\n",
    "print(f\"  Pickle: {time_pkl:.4f}s, Size: {size_pkl / 1024:.2f} KB\")\n",
    "\n",
    "# --- Pickle with Gzip Compression --- \n",
    "print(\"Serializing with Pickle + Gzip...\")\n",
    "start = time.perf_counter()\n",
    "with gzip.open(output_file_pkl_gz, 'wb') as f:\n",
    "    # Gzip works with file-like objects, pickle.dump writes to it\n",
    "    pickle.dump(large_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "time_pkl_gz = time.perf_counter() - start\n",
    "size_pkl_gz = os.path.getsize(output_file_pkl_gz)\n",
    "print(f\"  Pickle+Gzip: {time_pkl_gz:.4f}s, Size: {size_pkl_gz / 1024:.2f} KB\")\n",
    "\n",
    "# --- Decompression and Deserialization ---\n",
    "print(\"\\nDeserializing compressed file...\")\n",
    "start = time.perf_counter()\n",
    "with gzip.open(output_file_pkl_gz, 'rb') as f:\n",
    "    reconstructed_dict = pickle.load(f)\n",
    "time_des_gz = time.perf_counter() - start\n",
    "print(f\"  Pickle+Gzip Deserialization: {time_des_gz:.4f}s\")\n",
    "\n",
    "assert len(reconstructed_dict) == num_items\n",
    "print(\"  Data integrity verified.\")\n",
    "\n",
    "# --- Clean up files ---\n",
    "os.remove(output_file_pkl)\n",
    "os.remove(output_file_pkl_gz)\n",
    "print(f\"\\nCleaned up {output_file_pkl} and {output_file_pkl_gz}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialization for Specific Use Cases: Caching\n",
    "\n",
    "Dictionaries are perfect for caching results of expensive operations. Serialize the cache dictionary to disk (e.g., using Pickle) to persist it across script runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "\n",
    "cache_file = 'computation_cache.pkl'\n",
    "\n",
    "def expensive_calculation(param1, param2):\n",
    "    print(f\"  -> Performing expensive calculation for ({param1}, {param2})...\")\n",
    "    time.sleep(1) # Simulate work\n",
    "    return (param1 * param2) + param1 + param2\n",
    "\n",
    "def load_cache(filename):\n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            with open(filename, 'rb') as f:\n",
    "                print(f\"Loading cache from {filename}\")\n",
    "                return pickle.load(f)\n",
    "        except (pickle.UnpicklingError, EOFError, FileNotFoundError) as e:\n",
    "             print(f\"Cache file {filename} corrupted or empty, starting fresh. Error: {e}\")\n",
    "             return {}\n",
    "    else:\n",
    "        print(\"Cache file not found, creating new cache.\")\n",
    "        return {}\n",
    "\n",
    "def save_cache(cache, filename):\n",
    "     try:\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(cache, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(f\"Cache saved to {filename}\")\n",
    "     except Exception as e:\n",
    "        print(f\"Error saving cache to {filename}: {e}\")\n",
    "\n",
    "def cached_calculation(param1, param2, cache):\n",
    "    # Use a tuple of parameters as the dictionary key\n",
    "    cache_key = (param1, param2)\n",
    "    \n",
    "    if cache_key in cache:\n",
    "        print(f\"Cache hit for {cache_key}!\")\n",
    "        return cache[cache_key]\n",
    "    else:\n",
    "        print(f\"Cache miss for {cache_key}. Calculating...\")\n",
    "        result = expensive_calculation(param1, param2)\n",
    "        cache[cache_key] = result # Store result in cache\n",
    "        # In a real app, you might save the cache less frequently\n",
    "        # save_cache(cache, cache_file) \n",
    "        return result\n",
    "\n",
    "# --- Main execution --- \n",
    "calculation_cache = load_cache(cache_file)\n",
    "\n",
    "print(\"\\nFirst call (10, 5):\")\n",
    "res1 = cached_calculation(10, 5, calculation_cache)\n",
    "print(f\"Result: {res1}\")\n",
    "\n",
    "print(\"\\nSecond call (10, 5):\")\n",
    "res2 = cached_calculation(10, 5, calculation_cache)\n",
    "print(f\"Result: {res2}\")\n",
    "\n",
    "print(\"\\nFirst call (7, 3):\")\n",
    "res3 = cached_calculation(7, 3, calculation_cache)\n",
    "print(f\"Result: {res3}\")\n",
    "\n",
    "# Save the updated cache at the end\n",
    "save_cache(calculation_cache, cache_file)\n",
    "\n",
    "# Optional: Clean up cache file\n",
    "if os.path.exists(cache_file):\n",
    "    os.remove(cache_file)\n",
    "    print(f\"\\nCleaned up {cache_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle vs. HDF5: Performance Comparison\n",
    "\n",
    "While Pickle is fast for general Python objects, HDF5 (Hierarchical Data Format) is often used in scientific computing, especially for large numerical arrays (like NumPy arrays), potentially stored within dictionaries. It offers features Pickle lacks, like partial I/O and better memory efficiency for huge datasets.\n",
    "\n",
    "*Requires Installation:* `pip install h5py` (and potentially `numpy`)\n",
    "\n",
    "**Key Differences Summary (from provided text):**\n",
    "\n",
    "*   **Serialization Speed:** Pickle often faster, especially for non-numerical or mixed data.\n",
    "*   **Deserialization Speed:** Pickle often faster, but HDF5 can be faster for very large datasets or partial reads.\n",
    "*   **Memory Usage (Serialization):** HDF5 significantly better (doesn't need full copy in memory).\n",
    "*   **File Size:** HDF5 with compression can be much smaller for large numerical data.\n",
    "*   **Features:** HDF5 allows partial reads/writes, metadata handling, better cross-platform/language support. Pickle is Python-specific.\n",
    "\n",
    "**When to Consider HDF5 (over Pickle for dictionary values):**\n",
    "*   Dictionaries contain *very large* NumPy arrays.\n",
    "*   Memory is constrained during serialization.\n",
    "*   Need to read/write only parts of the data.\n",
    "*   Cross-language compatibility or long-term archiving is needed.\n",
    "\n",
    "**Note:** Directly serializing a complex *dictionary structure itself* might be better handled by Pickle or formats like MessagePack unless the *values* within the dictionary are large numerical arrays suited for HDF5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install h5py numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Requires h5py and numpy: pip install h5py numpy\n",
    "try:\n",
    "    import h5py\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import time\n",
    "    import os\n",
    "\n",
    "    # Create a dictionary where values are large NumPy arrays\n",
    "    array_size = (1000, 1000) # 1 million elements per array\n",
    "    num_arrays = 5\n",
    "    dict_with_arrays = {f'array_{i}': np.random.rand(*array_size) for i in range(num_arrays)}\n",
    "\n",
    "    print(f\"Created dictionary with {num_arrays} arrays of size {array_size}\\n\")\n",
    "\n",
    "    pickle_file = 'large_arrays.pkl'\n",
    "    hdf5_file = 'large_arrays.h5'\n",
    "\n",
    "    # --- Pickle Timing ---\n",
    "    start = time.perf_counter()\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(dict_with_arrays, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    pickle_write_time = time.perf_counter() - start\n",
    "    pickle_size = os.path.getsize(pickle_file)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        loaded_pickle = pickle.load(f)\n",
    "    pickle_read_time = time.perf_counter() - start\n",
    "    print(f\"Pickle Write: {pickle_write_time:.4f}s, Read: {pickle_read_time:.4f}s, Size: {pickle_size / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    # --- HDF5 Timing ---\n",
    "    start = time.perf_counter()\n",
    "    with h5py.File(hdf5_file, 'w') as f:\n",
    "        for key, value in dict_with_arrays.items():\n",
    "            f.create_dataset(key, data=value) # Store each array as a dataset\n",
    "            # Could also store metadata: f[key].attrs['timestamp'] = time.time()\n",
    "    hdf5_write_time = time.perf_counter() - start\n",
    "    hdf5_size = os.path.getsize(hdf5_file)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    loaded_hdf5 = {}\n",
    "    with h5py.File(hdf5_file, 'r') as f:\n",
    "        for key in f.keys():\n",
    "            loaded_hdf5[key] = f[key][:] # Load the full array data\n",
    "    hdf5_read_time = time.perf_counter() - start\n",
    "    print(f\"HDF5 Write:   {hdf5_write_time:.4f}s, Read: {hdf5_read_time:.4f}s, Size: {hdf5_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "    # --- HDF5 Partial Read Example ---\n",
    "    start = time.perf_counter()\n",
    "    with h5py.File(hdf5_file, 'r') as f:\n",
    "        # Read only the first 10x10 slice of the first array\n",
    "        partial_data = f['array_0'][:10, :10]\n",
    "    hdf5_partial_read_time = time.perf_counter() - start\n",
    "    print(f\"\\nHDF5 Partial Read (10x10 slice): {hdf5_partial_read_time:.6f}s\")\n",
    "    print(f\"  Shape of partial data: {partial_data.shape}\")\n",
    "\n",
    "    # Clean up\n",
    "    os.remove(pickle_file)\n",
    "    os.remove(hdf5_file)\n",
    "    print(f\"\\nCleaned up {pickle_file} and {hdf5_file}\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"h5py or numpy not installed. Run 'pip install h5py numpy' to run this cell.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during HDF5 comparison: {e}\")\n",
    "\n",
    "# Note: HDF5 shines more with even larger arrays or when memory during write is very constrained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "We've explored advanced serialization topics:\n",
    "\n",
    "*   Handling custom objects in JSON requires custom encoders/decoders.\n",
    "*   Circular references are handled by Pickle but problematic for others.\n",
    "*   Binary formats (Pickle, MessagePack, HDF5) often offer better performance/size than JSON for large data, with trade-offs.\n",
    "*   Techniques like incremental serialization and compression help manage very large datasets.\n",
    "*   Serialization is key for use cases like caching.\n",
    "*   HDF5 provides advantages over Pickle for specific scenarios involving large numerical arrays and partial I/O needs.\n",
    "\n",
    "**Next Steps:** The final notebook focuses on real-world applications, security considerations, and best practices for robust serialization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
